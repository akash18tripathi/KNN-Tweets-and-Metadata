{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSd3vc8oPt3b"
   },
   "source": [
    "## K-Nearest Neighbour\n",
    "\n",
    "\n",
    "### The Dataset\n",
    "The dataset is avaible in the zip file which is a collection of *11099 tweets*. The data will be in the form of a csv file. The ground truth is also given in the zip file which corresponds to whether a tweet was popular or not. Since the task involves selecting features yourself to vectorize a tweet , we suggest some data analysis of the columns you consider important.\n",
    "<br><br>\n",
    "\n",
    "### The Task\n",
    "You have to build a classifier which can predict the popularity of the tweet, i.e , if the tweet was popular or not. You are required to use **KNN** algorithm to build the classifier and cannot use any inbuilt classifier. All columns are supposed to be analyzed , filtered and preprocessed to determine its importance as a feature in the vector for every tweet (Not every column will be useful).<br>\n",
    "The Data contains the **raw text of the tweet**(in the text column) as well as other **meta data** like likes count , user followers count. Note that it might be useful to **create new columns** with useful information. For example, *number of hashtags* might be useful but is not directly present as a column.<br>\n",
    "There are 3 main sub parts:\n",
    "1. *Vectorize tweets using only meta data* - likes , user followers count , and other created data\n",
    "2. *Vectorize tweets using only it's text*. This segment will require NLP techniques to clean the text and extract a vector using a BoW model. Here is a useful link for the same - [Tf-Idf](https://towardsdatascience.com/text-vectorization-term-frequency-inverse-document-frequency-tfidf-5a3f9604da6d). Since these vectors will be very large , we recommend reducing their dimensinality (~10 - 25). Hint: [Dimentionality Reduction](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491). Please note that for this also you are allowed to use libraries.\n",
    "\n",
    "3. *Combining the vectors from above two techinques to create one bigger vector*\n",
    "<br>\n",
    "\n",
    "\n",
    "Using KNN on these vectors build a classifier to predict the popularity of the tweet and report accuracies on each of the three methods as well as analysis. You can use sklearn's Nearest Neighbors and need not write KNN from scratch. (However you cannot use the classifier directly). You are expected to try the classifier for different number of neighbors and identify the optimal K value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LJvylX8680U"
   },
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UrD1GJ6-YA5M"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/aakash/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/aakash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/aakash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoakVIVW7EOT"
   },
   "source": [
    "## Load and display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YRwXxW4WwEL8"
   },
   "outputs": [],
   "source": [
    "f = open('ground_truth.csv')\n",
    "label=[]\n",
    "for i in f.readlines():\n",
    "    label.append(int(i[0]))\n",
    "\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "df['label']=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>entities</th>\n",
       "      <th>metadata</th>\n",
       "      <th>source</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>...</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_followers_count</th>\n",
       "      <th>user_friends_count</th>\n",
       "      <th>user_listed_count</th>\n",
       "      <th>user_created_at</th>\n",
       "      <th>user_favourites_count</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>user_statuses_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tue Jul 31 13:34:34 +0000 2018</td>\n",
       "      <td>1.024290e+18</td>\n",
       "      <td>1.024290e+18</td>\n",
       "      <td>RT @EdwardTHardy: The 7th US Circuit Court of ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>113</td>\n",
       "      <td>...</td>\n",
       "      <td>Sherry Wahl</td>\n",
       "      <td>queenfancygirl</td>\n",
       "      <td>153</td>\n",
       "      <td>264</td>\n",
       "      <td>7</td>\n",
       "      <td>Thu Mar 18 19:16:31 +0000 2010</td>\n",
       "      <td>32984</td>\n",
       "      <td>False</td>\n",
       "      <td>31308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tue Jul 31 13:34:14 +0000 2018</td>\n",
       "      <td>1.024290e+18</td>\n",
       "      <td>1.024290e+18</td>\n",
       "      <td>RT @VenomMovie: The world has enough superhero...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'hashtags': [{'text': 'Venom', 'indices': [64...</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>5902</td>\n",
       "      <td>...</td>\n",
       "      <td>Kay Khairil ðŸŒ</td>\n",
       "      <td>ikaykhairil</td>\n",
       "      <td>780</td>\n",
       "      <td>382</td>\n",
       "      <td>12</td>\n",
       "      <td>Wed Mar 17 03:27:51 +0000 2010</td>\n",
       "      <td>6648</td>\n",
       "      <td>False</td>\n",
       "      <td>87272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tue Jul 31 13:34:40 +0000 2018</td>\n",
       "      <td>1.024290e+18</td>\n",
       "      <td>1.024290e+18</td>\n",
       "      <td>RT @FutbolBible: Teachers vs Students match &amp;a...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>3745</td>\n",
       "      <td>...</td>\n",
       "      <td>Charlie Hamilton</td>\n",
       "      <td>ch100897</td>\n",
       "      <td>255</td>\n",
       "      <td>246</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun Mar 03 09:23:03 +0000 2013</td>\n",
       "      <td>5426</td>\n",
       "      <td>False</td>\n",
       "      <td>1731</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue Jul 31 13:34:27 +0000 2018</td>\n",
       "      <td>1.024290e+18</td>\n",
       "      <td>1.024290e+18</td>\n",
       "      <td>RT @mashable: Someone from 'The Office' actual...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>Mike Santos</td>\n",
       "      <td>mikesantosx71</td>\n",
       "      <td>2419</td>\n",
       "      <td>2428</td>\n",
       "      <td>4</td>\n",
       "      <td>Thu May 25 14:37:29 +0000 2017</td>\n",
       "      <td>5993</td>\n",
       "      <td>False</td>\n",
       "      <td>2153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue Jul 31 13:34:28 +0000 2018</td>\n",
       "      <td>1.024290e+18</td>\n",
       "      <td>1.024290e+18</td>\n",
       "      <td>RT @_missj0hnson: Iâ€™m at Starbucks asking fo...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>{'iso_language_code': 'en', 'result_type': 're...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>25306</td>\n",
       "      <td>...</td>\n",
       "      <td>Soots</td>\n",
       "      <td>DaAverageDingus</td>\n",
       "      <td>314</td>\n",
       "      <td>722</td>\n",
       "      <td>7</td>\n",
       "      <td>Tue Mar 15 01:14:02 +0000 2011</td>\n",
       "      <td>6285</td>\n",
       "      <td>False</td>\n",
       "      <td>33503</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at            id        id_str  \\\n",
       "0  Tue Jul 31 13:34:34 +0000 2018  1.024290e+18  1.024290e+18   \n",
       "1  Tue Jul 31 13:34:14 +0000 2018  1.024290e+18  1.024290e+18   \n",
       "2  Tue Jul 31 13:34:40 +0000 2018  1.024290e+18  1.024290e+18   \n",
       "3  Tue Jul 31 13:34:27 +0000 2018  1.024290e+18  1.024290e+18   \n",
       "4  Tue Jul 31 13:34:28 +0000 2018  1.024290e+18  1.024290e+18   \n",
       "\n",
       "                                                text  truncated  \\\n",
       "0  RT @EdwardTHardy: The 7th US Circuit Court of ...      False   \n",
       "1  RT @VenomMovie: The world has enough superhero...      False   \n",
       "2  RT @FutbolBible: Teachers vs Students match &a...      False   \n",
       "3  RT @mashable: Someone from 'The Office' actual...      False   \n",
       "4  RT @_missj0hnson: Iâ€™m at Starbucks asking fo...      False   \n",
       "\n",
       "                                            entities  \\\n",
       "0  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "1  {'hashtags': [{'text': 'Venom', 'indices': [64...   \n",
       "2  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "3  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "4  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {'iso_language_code': 'en', 'result_type': 're...   \n",
       "1  {'iso_language_code': 'en', 'result_type': 're...   \n",
       "2  {'iso_language_code': 'en', 'result_type': 're...   \n",
       "3  {'iso_language_code': 'en', 'result_type': 're...   \n",
       "4  {'iso_language_code': 'en', 'result_type': 're...   \n",
       "\n",
       "                                              source  is_quote_status  \\\n",
       "0  <a href=\"http://twitter.com/download/android\" ...            False   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...            False   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...            False   \n",
       "3  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...            False   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...            False   \n",
       "\n",
       "   retweet_count  ...         user_name user_screen_name user_followers_count  \\\n",
       "0            113  ...       Sherry Wahl   queenfancygirl                  153   \n",
       "1           5902  ...  Kay Khairil ðŸŒ      ikaykhairil                  780   \n",
       "2           3745  ...  Charlie Hamilton         ch100897                  255   \n",
       "3             10  ...       Mike Santos    mikesantosx71                 2419   \n",
       "4          25306  ...             Soots  DaAverageDingus                  314   \n",
       "\n",
       "  user_friends_count  user_listed_count                 user_created_at  \\\n",
       "0                264                  7  Thu Mar 18 19:16:31 +0000 2010   \n",
       "1                382                 12  Wed Mar 17 03:27:51 +0000 2010   \n",
       "2                246                  1  Sun Mar 03 09:23:03 +0000 2013   \n",
       "3               2428                  4  Thu May 25 14:37:29 +0000 2017   \n",
       "4                722                  7  Tue Mar 15 01:14:02 +0000 2011   \n",
       "\n",
       "   user_favourites_count user_verified  user_statuses_count  label  \n",
       "0                  32984         False                31308      0  \n",
       "1                   6648         False                87272      0  \n",
       "2                   5426         False                 1731      1  \n",
       "3                   5993         False                 2153      0  \n",
       "4                   6285         False                33503      1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywNXO3TpwQkV"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "*This is an ungraded section but is recommended to get a good grasp on the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WAH13731wPS5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11099 entries, 0 to 11098\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   created_at             11099 non-null  object \n",
      " 1   id                     11099 non-null  float64\n",
      " 2   id_str                 11099 non-null  float64\n",
      " 3   text                   11099 non-null  object \n",
      " 4   truncated              11099 non-null  bool   \n",
      " 5   entities               11099 non-null  object \n",
      " 6   metadata               11099 non-null  object \n",
      " 7   source                 11099 non-null  object \n",
      " 8   is_quote_status        11099 non-null  bool   \n",
      " 9   retweet_count          11099 non-null  int64  \n",
      " 10  favorite_count         11099 non-null  int64  \n",
      " 11  lang                   11099 non-null  object \n",
      " 12  user_name              11099 non-null  object \n",
      " 13  user_screen_name       11099 non-null  object \n",
      " 14  user_followers_count   11099 non-null  int64  \n",
      " 15  user_friends_count     11099 non-null  int64  \n",
      " 16  user_listed_count      11099 non-null  int64  \n",
      " 17  user_created_at        11099 non-null  object \n",
      " 18  user_favourites_count  11099 non-null  int64  \n",
      " 19  user_verified          11099 non-null  bool   \n",
      " 20  user_statuses_count    11099 non-null  int64  \n",
      " 21  label                  11099 non-null  int64  \n",
      "dtypes: bool(3), float64(2), int64(8), object(9)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mztyk9Kew7q1"
   },
   "source": [
    "## Part-1\n",
    "*Vectorize tweets using only meta data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "y-rDkUtVxQph"
   },
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    \n",
    "    train_df = df[['is_quote_status','retweet_count','favorite_count','user_followers_count','user_friends_count','user_listed_count','user_favourites_count','user_verified','user_statuses_count']]\n",
    "    \n",
    "    for i in train_df.index:\n",
    "        if train_df['is_quote_status'][i]:\n",
    "            train_df['is_quote_status'][i]=1\n",
    "        else:\n",
    "            train_df['is_quote_status'][i]=0\n",
    "        if train_df['user_verified'][i]:\n",
    "            train_df['user_verified'][i]=1\n",
    "        else:\n",
    "            train_df['user_verified'][i]=0\n",
    "    X = train_df.iloc[:,:-1].values\n",
    "    y = train_df.iloc[:,-1].values \n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_271268/3746787076.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['is_quote_status'][i]=0\n",
      "/tmp/ipykernel_271268/3746787076.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['user_verified'][i]=0\n",
      "/tmp/ipykernel_271268/3746787076.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['is_quote_status'][i]=1\n",
      "/tmp/ipykernel_271268/3746787076.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['user_verified'][i]=1\n"
     ]
    }
   ],
   "source": [
    "features = get_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4K7Fzh5Q4wrV"
   },
   "source": [
    "Perform KNN using the vector obtained from get_features() function. Following are the steps to be followed:\n",
    "1. Normalise the vectors\n",
    "2. Split the data into training and test to estimate the performance.\n",
    "3. Fit the Nearest Neughbiurs module to the training data and obtain the predicted class by getting the nearest neighbours on the test data.\n",
    "4. Report the accuracy, chosen k-value and method used to obtain the predicted class. Hint: Plot accuracies for a range of k-values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EBHGbdsi4fe-"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(features)\n",
    "\n",
    "x = scaled\n",
    "y = df.iloc[:,-1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "knn = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(X_train)\n",
    "dist, indices = knn.kneighbors(X_test)\n",
    "y_pred=[]\n",
    "for i in range(len(X_test)):\n",
    "    d={0:0,1:1}\n",
    "    for j in range(k):\n",
    "        d[y_train[indices[i][j]]]+=1\n",
    "    Key_max = max(zip(d.values(), d.keys()))[1]  \n",
    "    y_pred.append(Key_max)\n",
    "y_pred= np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.940990990990991"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSufXze8zmV3"
   },
   "source": [
    "## Part-2\n",
    "Vectorize tweets based on the text. More details and reference links can be checked on the Tasks list in the start of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GBizLGhg0kQT"
   },
   "outputs": [],
   "source": [
    "def tweet_vectoriser(df):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    for i in range(len(df)):\n",
    "        df['text'][i]=re.sub(r'http\\S+', '', df['text'][i])\n",
    "        df['text'][i] = \" \".join(w for w in nltk.wordpunct_tokenize(df['text'][i]) if w.lower() in words or not w.isalpha())\n",
    "        word_tokens = nltk.tokenize.word_tokenize(df['text'][i])\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        sentence=\"\"\n",
    "        for j in filtered_sentence:\n",
    "            sentence += \" \"+ j\n",
    "        df['text'][i]=sentence\n",
    "\n",
    "    #TFIDF\n",
    "    vectoriser = TfidfVectorizer()\n",
    "    X = vectoriser.fit_transform(df['text'])\n",
    "    tf = np.array(X.toarray())\n",
    "    #PCA\n",
    "    pca = PCA(n_components=11)\n",
    "    tf_pca = pca.fit_transform(tf)\n",
    "    tf_pca = np.array(tf_pca)\n",
    "    \n",
    "    return tf_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q47SVkv9VzO"
   },
   "source": [
    "Perform KNN using the vector obtained from tweet_vectoriser() function. Following are the steps to be followed:\n",
    "\n",
    "1. Normalise the vectors\n",
    "2. Split the data into training and test to estimate the performance.\n",
    "3. Fit the Nearest Neughbiurs module to the training data and obtain the predicted class by getting the nearest neighbours on the test data.\n",
    "4. Report the accuracy, chosen k-value and method used to obtain the predicted class. Hint: Plot accuracies for a range of k-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7aDr7vAW-NX_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_271268/2691111578.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][i]=re.sub(r'http\\S+', '', df['text'][i])\n",
      "/tmp/ipykernel_271268/2691111578.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][i] = \" \".join(w for w in nltk.wordpunct_tokenize(df['text'][i]) if w.lower() in words or not w.isalpha())\n",
      "/tmp/ipykernel_271268/2691111578.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][i]=sentence\n"
     ]
    }
   ],
   "source": [
    "text_features = tweet_vectoriser(df)\n",
    "\n",
    "#Scaling\n",
    "scaled = scaler.fit_transform(text_features)\n",
    "x = scaled\n",
    "y = df.iloc[:,-1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "knn = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(X_train)\n",
    "dist, indices = knn.kneighbors(X_test)\n",
    "y_pred=[]\n",
    "for i in range(len(X_test)):\n",
    "    d={0:0,1:1}\n",
    "    for j in range(k):\n",
    "        d[y_train[indices[i][j]]]+=1\n",
    "    Key_max = max(zip(d.values(), d.keys()))[1]  \n",
    "    y_pred.append(Key_max)\n",
    "y_pred= np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8968468468468469"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg7hD8-O3PbO"
   },
   "source": [
    "## Part-3\n",
    "### Subpart-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bp9TVeVD9lKe"
   },
   "source": [
    "Combine both the vectors obtained from the tweet_vectoriser() and get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "r5ksyj7_3_xl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_271268/3746787076.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['is_quote_status'][i]=0\n",
      "/tmp/ipykernel_271268/3746787076.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['user_verified'][i]=0\n",
      "/tmp/ipykernel_271268/3746787076.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['is_quote_status'][i]=1\n",
      "/tmp/ipykernel_271268/3746787076.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['user_verified'][i]=1\n",
      "/tmp/ipykernel_271268/2691111578.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][i]=re.sub(r'http\\S+', '', df['text'][i])\n",
      "/tmp/ipykernel_271268/2691111578.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][i] = \" \".join(w for w in nltk.wordpunct_tokenize(df['text'][i]) if w.lower() in words or not w.isalpha())\n",
      "/tmp/ipykernel_271268/2691111578.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'][i]=sentence\n"
     ]
    }
   ],
   "source": [
    "features = get_features(df)\n",
    "text_features = tweet_vectoriser(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11099, 19) (11099,)\n"
     ]
    }
   ],
   "source": [
    "x = np.column_stack([features,text_features])\n",
    "y = df.iloc[:,-1].values\n",
    "\n",
    "print(np.shape(x),np.shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swCD5Jp69xo5"
   },
   "source": [
    "Perform KNN using the vector obtained in the previous step. Following are the steps to be followed:\n",
    "\n",
    "1. Normalise the vectors\n",
    "2. Split the data into training and test to estimate the performance.\n",
    "3. Fit the Nearest Neughbiurs module to the training data and obtain the predicted class by getting the nearest neighbours on the test data.\n",
    "4. Report the accuracy, chosen k-value and method used to obtain the predicted class. Hint: Plot accuracies for a range of k-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rufknWgo4AvY"
   },
   "outputs": [],
   "source": [
    "scaled = scaler.fit_transform(x)\n",
    "\n",
    "x = scaled\n",
    "y = df.iloc[:,-1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=5\n",
    "knn = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(X_train)\n",
    "dist, indices = knn.kneighbors(X_test)\n",
    "y_pred=[]\n",
    "for i in range(len(X_test)):\n",
    "    d={0:0,1:1}\n",
    "    for j in range(k):\n",
    "        d[y_train[indices[i][j]]]+=1\n",
    "    Key_max = max(zip(d.values(), d.keys()))[1]  \n",
    "    y_pred.append(Key_max)\n",
    "y_pred= np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NVQLLhE3c79"
   },
   "source": [
    "### Subpart-2\n",
    "\n",
    "Explain the differences between the accuracies obtained in each part above based on the features used.\n",
    "\n",
    "Part1- accuracy is 94%, as the data is numeric and can be scaled the absolute values, its predicted better\n",
    "Part2- Accuracy is 89%, as the data is text and PDA is applied, some info can be lost in reduction process, thus, a bit less accuracy\n",
    "Part3 - 93% accuracy, as the data is numeric+text, its a good blend of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkiTeqaE_4ic"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
